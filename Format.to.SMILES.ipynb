{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format input data into SMILEs\n",
    "\n",
    "11th November 2025\n",
    "\n",
    "\n",
    "This notebook is for reformatting any input file into SMILES strings.\n",
    "\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/fsx/alex/MolecularTypeClassifier-1/.pixi/envs/default/bin/python\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import sys, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "print(sys.executable)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import sqlite3\n",
    "import os \n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "#from mol_utils import MolecularClassifier\n",
    "from rdkit import Chem, RDLogger\n",
    "import pyarrow\n",
    "import fastparquet\n",
    "import cirpy\n",
    "\n",
    "\n",
    "from mol_utils import classify_smiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local setup for data imports\n",
    "\n",
    "scratch_dir = Path(\"scratch\")\n",
    "scratch_dir.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loader utilities ready.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import tarfile\n",
    "from typing import List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def _log_info(message: str) -> None:\n",
    "    \"\"\"Write an informational message using tqdm.\"\"\"\n",
    "    tqdm.write(message)\n",
    "\n",
    "\n",
    "def _log_warning(message: str) -> None:\n",
    "    \"\"\"Write a warning message using tqdm.\"\"\"\n",
    "    tqdm.write(f\"WARNING: {message}\")\n",
    "\n",
    "\n",
    "# Optional RDKit import for SDF handling\n",
    "try:\n",
    "    from rdkit import Chem  # type: ignore\n",
    "    _HAS_RDKIT = True\n",
    "except Exception as rdkit_exc:  # pragma: no cover\n",
    "    _log_warning(f\"RDKit not available: {rdkit_exc}. SDF parsing will not work.\")\n",
    "    Chem = None  # type: ignore\n",
    "    _HAS_RDKIT = False\n",
    "\n",
    "\n",
    "def read_csv_column(path: Path, column: str) -> pd.DataFrame:\n",
    "    \"\"\"Read a CSV and return only the requested column as a DataFrame.\"\"\"\n",
    "    _log_info(f\"Loading CSV: {path} (column={column})\")\n",
    "    df = pd.read_csv(path)\n",
    "    if column not in df.columns:\n",
    "        raise KeyError(f\"Column '{column}' not found in {path}. Available: {list(df.columns)}\")\n",
    "    return df[[column]].dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "def read_xls_column(path: Path, column: str) -> pd.DataFrame:\n",
    "    \"\"\"Read an Excel file and return only the requested column as a DataFrame.\"\"\"\n",
    "    _log_info(f\"Loading Excel: {path} (column={column})\")\n",
    "    df = pd.read_excel(path)\n",
    "    if column not in df.columns:\n",
    "        raise KeyError(f\"Column '{column}' not found in {path}. Available: {list(df.columns)}\")\n",
    "    return df[[column]].dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "def read_dat_delimited_column(path: Path, column: str, sep: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"Read a .dat file and return only the requested column.\n",
    "\n",
    "    Tries tab, comma, and pipe separators if none is provided.\n",
    "    \"\"\"\n",
    "    _log_info(f\"Loading DAT: {path} (column={column})\")\n",
    "    seps = [sep] if sep else [\"\\t\", \",\", \"|\"]\n",
    "    last_err: Optional[Exception] = None\n",
    "    for s in seps:\n",
    "        try:\n",
    "            df = pd.read_csv(path, sep=s)\n",
    "            if column in df.columns:\n",
    "                return df[[column]].dropna().reset_index(drop=True)\n",
    "            last_err = KeyError(f\"Column '{column}' not in columns parsed with sep '{s}'\")\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    raise RuntimeError(f\"Failed to parse {path} for column '{column}'. Last error: {last_err}\")\n",
    "\n",
    "\n",
    "def read_sdf_smiles(path: Path, smiles_field: str = \"canonical_smiles\") -> pd.DataFrame:\n",
    "    \"\"\"Read an SDF file and extract SMILES from a specified property or from the molecule.\"\"\"\n",
    "    if not _HAS_RDKIT:\n",
    "        raise ImportError(\"RDKit is required to parse SDF files. Please install rdkit-pypi.\")\n",
    "    _log_info(f\"Loading SDF: {path} (field={smiles_field})\")\n",
    "    supplier = Chem.SDMolSupplier(str(path), removeHs=False)  # type: ignore[attr-defined]\n",
    "    smiles_list: List[str] = []\n",
    "    for mol in tqdm(supplier, desc=f\"Reading {path.name}\"):\n",
    "        if mol is None:\n",
    "            continue\n",
    "        value = mol.GetProp(smiles_field) if mol.HasProp(smiles_field) else None\n",
    "        if not value:\n",
    "            try:\n",
    "                value = Chem.MolToSmiles(mol)  # type: ignore[attr-defined]\n",
    "            except Exception:\n",
    "                value = None\n",
    "        if value:\n",
    "            smiles_list.append(value)\n",
    "    return pd.DataFrame({\"canonical_smiles\": smiles_list}).dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "def read_glycan_csv_fourth_column(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Read a CSV and return the 4th column (index 3).\"\"\"\n",
    "    _log_info(f\"Loading Glycan CSV: {path} (4th column)\")\n",
    "    df = pd.read_csv(path, header=0)\n",
    "    if df.shape[1] < 4:\n",
    "        raise ValueError(f\"Expected at least 4 columns in {path}, got {df.shape[1]}\")\n",
    "    col_name = df.columns[3]\n",
    "    out = df[[col_name]].dropna().reset_index(drop=True)\n",
    "    out.columns = [\"glycan_field\"]\n",
    "    return out\n",
    "\n",
    "def extract_field(data, field):\n",
    "    if isinstance(data, dict):\n",
    "        for k, v in data.items():\n",
    "            if k == field and v:\n",
    "                yield v\n",
    "            else:\n",
    "                yield from extract_field(v, field)\n",
    "    elif isinstance(data, list):\n",
    "        for item in data:\n",
    "            yield from extract_field(item, field)\n",
    "\n",
    "def read_rips_from_tar_json(path: Path, json_field: str = \"translation\") -> pd.DataFrame:\n",
    "    values = []\n",
    "    with tarfile.open(path, \"r\") as tf:\n",
    "        members = [m for m in tf.getmembers() if m.isfile() and m.name.lower().endswith(\".json\")]\n",
    "        for m in tqdm(members, desc=f\"Reading {path.name}\"):\n",
    "            f = tf.extractfile(m)\n",
    "            if f is None:\n",
    "                continue\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                for val in extract_field(data, json_field):\n",
    "                    values.append(str(val))\n",
    "            except Exception as e:\n",
    "                _log_warning(f\"Failed to parse {m.name} inside {path.name}: {e}\")\n",
    "            finally:\n",
    "                f.close()\n",
    "    return pd.DataFrame({json_field: values}).dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "_log_info(\"Data loader utilities ready.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Functions to convert different inputs to SMILES\n",
    "\n",
    "\n",
    "#########################################\n",
    "#                                       #\n",
    "#        For canonical peptides         #\n",
    "#                                       #\n",
    "#########################################\n",
    "\n",
    "# pip install rdkit-pypi\n",
    "from rdkit import Chem, RDLogger\n",
    "\n",
    "# Silence RDKit warnings\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "def seq_to_smiles(seq: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Convert a DNA/RNA/canonical peptides sequence string to canonical SMILES using RDKit.\n",
    "    Handles FASTA headers, whitespace, and is case-insensitive.\n",
    "    \"\"\"\n",
    "    if not isinstance(seq, str) or not seq.strip():\n",
    "        return None\n",
    "    # Clean input: strip FASTA headers and whitespace\n",
    "    s = \"\".join(ln.strip() for ln in seq.splitlines() if not ln.startswith(\">\"))\n",
    "    s = s.replace(\" \", \"\").replace(\"\\t\", \"\").upper()\n",
    "    try:\n",
    "        mol = Chem.MolFromSequence(s)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        Chem.SanitizeMol(mol)\n",
    "        # directly call MolToSmiles from rdmolfiles for canonical SMILES\n",
    "        return Chem.rdmolfiles.MolToSmiles(mol)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "#########################################\n",
    "#                                       #\n",
    "#     For non-canonical peptides        #\n",
    "#                                       #\n",
    "#########################################\n",
    "\n",
    "# pip install rdkit-pypi\n",
    "from rdkit import Chem, RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "# Standard 20 amino acids (one-letter)\n",
    "_AA20 = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "\n",
    "# Common noncanonical / ambiguous mappings â†’ standard\n",
    "# U: selenocysteineâ†’C, O: pyrrolysineâ†’K, B: Asxâ†’N, Z: Glxâ†’Q, J: Leu/Ileâ†’L, X: unknownâ†’G\n",
    "_NONCANON_MAP = {\n",
    "    'U': 'C',  # selenocysteine\n",
    "    'O': 'K',  # pyrrolysine\n",
    "    'B': 'N',  # Asx (D/N)\n",
    "    'Z': 'Q',  # Glx (E/Q)\n",
    "    'J': 'L',  # Leu/Ile\n",
    "    'X': 'G',  # unknown -> glycine\n",
    "}\n",
    "\n",
    "def _strip_fasta_headers(seq: str) -> str:\n",
    "    return \"\".join(ln.strip() for ln in str(seq).splitlines() if ln and not ln.startswith(\">\"))\n",
    "\n",
    "def noncanonical_peptide_seq_to_smiles(seq: str, mode: str = \"lenient\") -> str | None:\n",
    "    \"\"\"\n",
    "    Convert a non-canonical peptide sequence to canonical SMILES using RDKit.\n",
    "    - mode=\"lenient\": map known noncanonicals; drop any remaining unknown letters.\n",
    "    - mode=\"strict\":  fail (return None) if any unknown letters remain after mapping.\n",
    "    \"\"\"\n",
    "    if not isinstance(seq, str) or not seq.strip():\n",
    "        return None\n",
    "\n",
    "    # Clean: remove FASTA headers/whitespace; keep letters only, uppercase\n",
    "    s = _strip_fasta_headers(seq)\n",
    "    s = \"\".join(ch for ch in s if ch.isalpha()).upper()\n",
    "    if not s:\n",
    "        return None\n",
    "\n",
    "    # Normalize: map common noncanonicals/ambiguities\n",
    "    out_chars = []\n",
    "    unknown_seen = False\n",
    "    for ch in s:\n",
    "        if ch in _AA20:\n",
    "            out_chars.append(ch)\n",
    "        elif ch in _NONCANON_MAP:\n",
    "            out_chars.append(_NONCANON_MAP[ch])\n",
    "        else:\n",
    "            # unknown symbol (e.g., modified residues encoded as letters)\n",
    "            if mode == \"strict\":\n",
    "                return None\n",
    "            unknown_seen = True\n",
    "            # lenient: drop it\n",
    "            continue\n",
    "\n",
    "    norm = \"\".join(out_chars)\n",
    "    if not norm:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # RDKit peptide builders\n",
    "        mol = Chem.MolFromSequence(norm)\n",
    "        if mol is None:\n",
    "            mol = Chem.MolFromFASTA(norm)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        Chem.SanitizeMol(mol)\n",
    "        # Canonical SMILES\n",
    "        return Chem.rdmolfiles.MolToSmiles(mol)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########################################\n",
    "#                                       #\n",
    "#             For glycans               #\n",
    "#                                       #\n",
    "#########################################\n",
    "\n",
    "\n",
    "# pip install requests tqdm\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    tqdm = None\n",
    "\n",
    "GLYCOSMOS_BASE = \"https://api.glycosmos.org/glycanformatconverter/2.10.4\"\n",
    "CONVERT_PATH = \"/wurcs2iupaccondensed\"\n",
    "\n",
    "def _is_wurcs(s: str) -> bool:\n",
    "    return isinstance(s, str) and s.strip().upper().startswith(\"WURCS=\")\n",
    "\n",
    "def _session_with_retries(total=5, backoff=0.5) -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=total,\n",
    "        backoff_factor=backoff,\n",
    "        status_forcelist=(429, 500, 502, 503, 504),\n",
    "        allowed_methods=frozenset([\"POST\"]),\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry, pool_connections=100, pool_maxsize=100)\n",
    "    s.mount(\"http://\", adapter)\n",
    "    s.mount(\"https://\", adapter)\n",
    "    return s\n",
    "\n",
    "def _post_batch(wurcs_batch, session: requests.Session, timeout: float = 30.0) -> list[str | None]:\n",
    "    \"\"\"\n",
    "    Send a batch (list) of WURCS strings.\n",
    "    Returns a list aligned to the input batch, each item is IUPAC string or None.\n",
    "    \"\"\"\n",
    "    url = f\"{GLYCOSMOS_BASE}{CONVERT_PATH}\"\n",
    "    for attempt in range(3):  # light manual retries on top of session retries\n",
    "        try:\n",
    "            r = session.post(url, json=wurcs_batch, timeout=timeout)\n",
    "            if r.status_code == 413 and len(wurcs_batch) > 1:\n",
    "                # Payload too large: split and recurse.\n",
    "                mid = len(wurcs_batch) // 2\n",
    "                return (_post_batch(wurcs_batch[:mid], session, timeout) +\n",
    "                        _post_batch(wurcs_batch[mid:], session, timeout))\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "            out = []\n",
    "            for obj in data if isinstance(data, list) else []:\n",
    "                out.append(obj.get(\"IUPACcondensed\") if isinstance(obj, dict) else None)\n",
    "            if len(out) != len(wurcs_batch):\n",
    "                out = (out + [None] * len(wurcs_batch))[:len(wurcs_batch)]\n",
    "            return out\n",
    "        except Exception:\n",
    "            time.sleep(0.4 * (attempt + 1))\n",
    "    return [None] * len(wurcs_batch)\n",
    "\n",
    "def glycans_wurcs_to_cols_fast(\n",
    "    df: pd.DataFrame,\n",
    "    colname: str = \"glycan_field\",\n",
    "    smiles_col: str = \"smiles\",\n",
    "    iupac_col: str = \"iupac_condensed\",\n",
    "    chunk_size: int = 200,\n",
    "    max_workers: int = 4,\n",
    "    cache: dict[str, str | None] | None = None,\n",
    "    show_progress: bool = True,           # ðŸ‘ˆ enable/disable progress bar\n",
    "    progress_desc: str = \"Converting WURCSâ†’IUPAC\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Faster conversion with batching, optional parallelism, caching, and a tqdm progress bar.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    series = out[colname].astype(\"string\", copy=False)\n",
    "\n",
    "    # Collect unique, valid WURCS\n",
    "    unique_valid = [w for w in pd.Series(series.unique()) if _is_wurcs(w)]\n",
    "    if not unique_valid:\n",
    "        out[iupac_col] = None\n",
    "        out[smiles_col] = None\n",
    "        return out\n",
    "\n",
    "    # Initialize / reuse cache\n",
    "    local_cache: dict[str, str | None] = {} if cache is None else cache\n",
    "    to_query = [w for w in unique_valid if w not in local_cache]\n",
    "    if not to_query:\n",
    "        out[iupac_col] = series.map(lambda w: local_cache.get(w) if _is_wurcs(w) else None)\n",
    "        out[smiles_col] = None\n",
    "        return out\n",
    "\n",
    "    batches = [to_query[i:i + chunk_size] for i in range(0, len(to_query), chunk_size)]\n",
    "    session = _session_with_retries()\n",
    "\n",
    "    # Prepare progress bar (counts items, not batches)\n",
    "    pbar = None\n",
    "    if show_progress and tqdm is not None:\n",
    "        pbar = tqdm(total=len(to_query), desc=progress_desc, unit=\"glycan\")\n",
    "\n",
    "    def _record_batch(batch, results):\n",
    "        for w, iupac in zip(batch, results):\n",
    "            local_cache[w] = iupac\n",
    "        if pbar is not None:\n",
    "            # advance by the number of items processed in this batch\n",
    "            pbar.update(len(batch))\n",
    "\n",
    "    if max_workers and max_workers > 1 and len(batches) > 1:\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            future_map = {ex.submit(_post_batch, b, session): b for b in batches}\n",
    "            for fut in as_completed(future_map):\n",
    "                b = future_map[fut]\n",
    "                res = fut.result()\n",
    "                _record_batch(b, res)\n",
    "    else:\n",
    "        for b in batches:\n",
    "            res = _post_batch(b, session)\n",
    "            _record_batch(b, res)\n",
    "\n",
    "    if pbar is not None:\n",
    "        pbar.close()\n",
    "\n",
    "    # Map back to rows\n",
    "    out[iupac_col] = series.map(lambda w: local_cache.get(w) if _is_wurcs(w) else None)\n",
    "    out[smiles_col] = None\n",
    "    return out\n",
    "\n",
    "\n",
    "# pip install requests\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "GLYCOSMOS_BASE = \"https://api.glycosmos.org/glycanformatconverter/2.10.4\"\n",
    "\n",
    "def _is_wurcs(s: str) -> bool:\n",
    "    return isinstance(s, str) and s.strip().upper().startswith(\"WURCS=\")\n",
    "\n",
    "def _wurcs_to_iupac(wurcs: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Convert a single WURCS string to IUPAC-condensed using Glycosmos.\n",
    "    Returns IUPAC string or None on failure.\n",
    "    \"\"\"\n",
    "    if not _is_wurcs(wurcs):\n",
    "        return None\n",
    "    try:\n",
    "        # API accepts a JSON array of WURCS strings; we send one item\n",
    "        url = f\"{GLYCOSMOS_BASE}/wurcs2iupaccondensed\"\n",
    "        r = requests.post(url, json=[wurcs], timeout=60)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        if isinstance(data, list) and data and isinstance(data[0], dict):\n",
    "            return data[0].get(\"IUPACcondensed\")\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def glycans_wurcs_to_cols(df: pd.DataFrame,\n",
    "                          colname: str = \"glycan_field\",\n",
    "                          smiles_col: str = \"smiles\",\n",
    "                          iupac_col: str = \"iupac_condensed\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For a DataFrame with WURCS in `colname`:\n",
    "      - Writes IUPAC-condensed to `iupac_condensed`\n",
    "      - Sets `smiles` to None (placeholder until you add an IUPACâ†’SMILES tool)\n",
    "    Keeps rows as-is; no drops.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    # IUPAC from WURCS\n",
    "    out[iupac_col] = out[colname].apply(_wurcs_to_iupac)\n",
    "    # No direct SMILES yet without a glycan IUPACâ†’SMILES library\n",
    "    out[smiles_col] = None\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset imports...\n",
      "Loading CSV: /fsx/data/raw/drugbank/DrugBank_SM_drugs.csv (column=moldb_smiles)\n",
      "Loading CSV: /fsx/data/raw/DNA.RNA_seq/random_DNA_RNA_sequences_10000.csv (column=Sequence)\n",
      "Loading Excel: /fsx/data/raw/TPDB/main.xlsx (column=Sequence)\n",
      "Loading DAT: /fsx/data/raw/NCPbook_noncanonicals/NCP.book_Homo_sapiens.dat (column=NCP_sequence)\n",
      "Loading CSV: /fsx/data/raw/CycPeptMPDB/CycPeptMPDB_Peptide_Shape_Lariat.csv (column=SMILES)\n",
      "Loading CSV: /fsx/data/raw/CycPeptMPDB/CycPeptMPDB_Peptide_Shape_Circle.csv (column=SMILES)\n",
      "Loading SDF: /fsx/data/raw/supernatural/supernatural3-11-2025.sdf (field=canonical_smiles)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12504842aa2426daf2603a75ecca87f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading supernatural3-11-2025.sdf:   0%|          | 0/121636 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glycan CSV: /fsx/data/raw/Glytoucan/glycan.csv (4th column)\n",
      "Loading RIPs from TAR JSON: /fsx/data/raw/mibig/mibig_json_4.0.tar (field=translation)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3038d3dd122547e8a3e03c5916028711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading mibig_json_4.0.tar:   0%|          | 0/3013 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import summary: {'df_sm': 13335, 'df_oligos': 10000, 'df_can_peptides': 58583, 'df_noncan_peptides': 31806, 'df_cyclic_pep_lariat': 2936, 'df_cyclic_pep_circle': 5530, 'df_nat_prod': 121636, 'df_glycans': 253440, 'df_rips': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'df_sm': 13335,\n",
       " 'df_oligos': 10000,\n",
       " 'df_can_peptides': 58583,\n",
       " 'df_noncan_peptides': 31806,\n",
       " 'df_cyclic_pep_lariat': 2936,\n",
       " 'df_cyclic_pep_circle': 5530,\n",
       " 'df_nat_prod': 121636,\n",
       " 'df_glycans': 253440,\n",
       " 'df_rips': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, import all input files required\n",
    "# Define paths\n",
    "SM = Path(\"/fsx/data/raw/drugbank/DrugBank_SM_drugs.csv\")\n",
    "oligos = Path(\"/fsx/data/raw/DNA.RNA_seq/random_DNA_RNA_sequences_10000.csv\")\n",
    "can_peptides = Path(\"/fsx/data/raw/TPDB/main.xlsx\")\n",
    "noncan_peptides = Path(\"/fsx/data/raw/NCPbook_noncanonicals/NCP.book_Homo_sapiens.dat\")\n",
    "cyclic_pep_lariat = Path(\"/fsx/data/raw/CycPeptMPDB/CycPeptMPDB_Peptide_Shape_Lariat.csv\")\n",
    "cyclic_pep_circle = Path(\"/fsx/data/raw/CycPeptMPDB/CycPeptMPDB_Peptide_Shape_Circle.csv\")\n",
    "nat_prod = Path(\"/fsx/data/raw/supernatural/supernatural3-11-2025.sdf\")\n",
    "glycans = Path(\"/fsx/data/raw/GlycoShape/GlycoShape_smiles.csv\")\n",
    "RIPs = Path(\"/fsx/data/raw/mibig/mibig_json_4.0.tar\")\n",
    "\n",
    "# Load datasets into DataFrames\n",
    "_log_info(\"Starting dataset imports...\")\n",
    "\n",
    "# Approved drugs (SMs): column 'moldb_smiles'\n",
    "df_sm = read_csv_column(SM, \"moldb_smiles\")\n",
    "\n",
    "# Oligos/Nucleotides: column 'Sequence'\n",
    "df_oligos = read_csv_column(oligos, \"Sequence\")\n",
    "df_oligos[\"smiles\"] = df_oligos[\"Sequence\"].apply(seq_to_smiles)\n",
    "\n",
    "# Peptides (canonical): column 'Sequence'\n",
    "df_can_peptides = read_xls_column(can_peptides, \"Sequence\")\n",
    "df_can_peptides[\"smiles\"] = df_can_peptides[\"Sequence\"].apply(seq_to_smiles)\n",
    "\n",
    "# Non-canonical peptides: column 'NCP_sequence'\n",
    "df_noncan_peptides = read_dat_delimited_column(noncan_peptides, \"NCP_sequence\")\n",
    "df_noncan_peptides[\"smiles\"] = df_noncan_peptides[\"NCP_sequence\"].apply(\n",
    "    lambda s: noncanonical_peptide_seq_to_smiles(s, mode=\"lenient\")\n",
    ")\n",
    "\n",
    "# CycPeptMPDB (Macrocycles-lariat): column 'SMILES'\n",
    "df_cyclic_pep_lariat = read_csv_column(cyclic_pep_lariat, \"SMILES\")\n",
    "\n",
    "# CycPeptMPDB (Macrocycles-circular): column 'SMILES'\n",
    "df_cyclic_pep_circle = read_csv_column(cyclic_pep_circle, \"SMILES\")\n",
    "\n",
    "# Natural products: SDF field 'canonical_smiles'\n",
    "df_nat_prod = read_sdf_smiles(nat_prod, smiles_field=\"canonical_smiles\")\n",
    "\n",
    "# Glycans: 4th column in GlyTouCan\n",
    "df_glycans = read_csv_column(glycans, \"smiles\")\n",
    "\n",
    "# RIPs: 'translation' field in each JSON inside tar\n",
    "df_rips = read_rips_from_tar_json(RIPs, json_field=\"translation\")\n",
    "df_rips[\"smiles\"] = df_rips[\"translation\"].apply(\n",
    "    lambda s: noncanonical_peptide_seq_to_smiles(s, mode=\"lenient\")\n",
    ")\n",
    "\n",
    "# Basic summary\n",
    "summary = {\n",
    "    \"df_sm\": len(df_sm),\n",
    "    \"df_oligos\": len(df_oligos),\n",
    "    \"df_can_peptides\": len(df_can_peptides),\n",
    "    \"df_noncan_peptides\": len(df_noncan_peptides),\n",
    "    \"df_cyclic_pep_lariat\": len(df_cyclic_pep_lariat),\n",
    "    \"df_cyclic_pep_circle\": len(df_cyclic_pep_circle),\n",
    "    \"df_nat_prod\": len(df_nat_prod),\n",
    "    \"df_glycans\": len(df_glycans),\n",
    "    \"df_rips\": len(df_rips),\n",
    "}\n",
    "_log_info(f\"Import summary: {summary}\")\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save intermediary outputs just in case\n",
    "#df_oligos.to_csv(\"Pos.ctrl_oligos.csv\", index=False);\n",
    "#df_can_peptides.to_csv(\"Pos.ctrl_CanonicalPeptides.csv\", index=False);\n",
    "#df_noncan_peptides.to_csv(\"Pos.ctrl_NoncanonicalPeptides.csv\", index=False);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Pivot longer to merge the dataframes \n",
    "#Use the file name (without the \"df_\" prefix as a unique identifier -> with unique identifier to \"Class\" column & values to \"SMILES\" column\n",
    "#Dataframes to be merged: df_sm, df_oligos, df_can_peptides, df_noncan_peptides, df_cyclic_pep_lariat, df_cyclic_pep_circle, df_nat_prod, df_glycans\n",
    "def pivot_longer_dfs():\n",
    "    dfs = []\n",
    "\n",
    "    # Use 1st column for these\n",
    "    for name, df in {\n",
    "        \"sm\": df_sm,\n",
    "        \"cyclic_pep_lariat\": df_cyclic_pep_lariat,\n",
    "        \"cyclic_pep_circle\": df_cyclic_pep_circle,\n",
    "        \"nat_prod\": df_nat_prod,\n",
    "        \"glycans\": df_glycans,\n",
    "\n",
    "    }.items():\n",
    "        col = df.columns[0]\n",
    "        dfs.append(pd.DataFrame({\n",
    "            \"Class\": name,\n",
    "            \"SMILES\": df[col]\n",
    "        }))\n",
    "\n",
    "    # Use 2nd column for these\n",
    "    for name, df in {\n",
    "        \"oligos\": df_oligos,\n",
    "        \"can_peptides\": df_can_peptides,\n",
    "        \"noncan_peptides\": df_noncan_peptides,\n",
    "        \"rips\": df_rips,\n",
    "    }.items():\n",
    "        col = df.columns[1]\n",
    "        dfs.append(pd.DataFrame({\n",
    "            \"Class\": name,\n",
    "            \"SMILES\": df[col]\n",
    "        }))\n",
    "\n",
    "    # Combine all into one long table\n",
    "    merged = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Drop missing or empty SMILES if desired\n",
    "    merged = merged.dropna(subset=[\"SMILES\"])\n",
    "    merged = merged[merged[\"SMILES\"].astype(str).str.strip() != \"\"].reset_index(drop=True)\n",
    "\n",
    "    return merged\n",
    "\n",
    "# ---- Run the function ----\n",
    "df_merged = pivot_longer_dfs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class                                             SMILES\n",
      "0    sm  CC[C@H](C)[C@H](NC(=O)[C@H](CCC(O)=O)NC(=O)[C@...\n",
      "1    sm  CC(C)C[C@H](NC(=O)[C@@H](COC(C)(C)C)NC(=O)[C@H...\n",
      "2    sm  CC(C)C[C@@H](NC(=O)CNC(=O)[C@@H](NC=O)C(C)C)C(...\n",
      "3    sm  NC(=O)CC[C@@H]1NC(=O)[C@H](CC2=CC=CC=C2)NC(=O)...\n",
      "4    sm  CC(C)C[C@H](NC(=O)[C@@H](CCCNC(N)=O)NC(=O)[C@H...\n",
      "\n",
      "Summary by Class:\n",
      "Class\n",
      "nat_prod             121636\n",
      "can_peptides          56300\n",
      "noncan_peptides       31806\n",
      "sm                    13335\n",
      "cyclic_pep_circle      5530\n",
      "oligos                 5087\n",
      "cyclic_pep_lariat      2936\n",
      "glycans                2697\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df_merged.head(5))\n",
    "print(\"\\nSummary by Class:\")\n",
    "print(df_merged[\"Class\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv(\"Pos.ctrl_Molecules_merged.csv\", index=False);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
