{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "from pathlib import Path\n",
        "from loguru import logger\n",
        "from tqdm import tqdm\n",
        "\n",
        "scratch_dir = Path(\"scratch\")\n",
        "scratch_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "logger.add(scratch_dir / \"analysis.log\", rotation=\"1 MB\", retention=3)\n",
        "logger.info(\"Notebook started\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import tarfile\n",
        "from pathlib import Path\n",
        "from typing import List, Optional\n",
        "\n",
        "import pandas as pd\n",
        "from loguru import logger\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Optional RDKit import for SDF handling\n",
        "try:\n",
        "    from rdkit import Chem  # type: ignore\n",
        "    _HAS_RDKIT = True\n",
        "except Exception as rdkit_exc:  # pragma: no cover\n",
        "    logger.warning(\"RDKit not available: {}. SDF parsing will not work.\", rdkit_exc)\n",
        "    Chem = None  # type: ignore\n",
        "    _HAS_RDKIT = False\n",
        "\n",
        "\n",
        "def read_csv_column(path: Path, column: str) -> pd.DataFrame:\n",
        "    \"\"\"Read a CSV and return only the requested column as a DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    path : Path\n",
        "        Path to the CSV file.\n",
        "    column : str\n",
        "        Column name to select.\n",
        "    \"\"\"\n",
        "    logger.info(\"Loading CSV: {} (column={})\", path, column)\n",
        "    df = pd.read_csv(path)\n",
        "    if column not in df.columns:\n",
        "        raise KeyError(f\"Column '{column}' not found in {path}. Available: {list(df.columns)}\")\n",
        "    return df[[column]].dropna().reset_index(drop=True)\n",
        "\n",
        "\n",
        "def read_xls_column(path: Path, column: str) -> pd.DataFrame:\n",
        "    \"\"\"Read an Excel file and return only the requested column as a DataFrame.\"\"\"\n",
        "    logger.info(\"Loading Excel: {} (column={})\", path, column)\n",
        "    df = pd.read_excel(path)\n",
        "    if column not in df.columns:\n",
        "        raise KeyError(f\"Column '{column}' not found in {path}. Available: {list(df.columns)}\")\n",
        "    return df[[column]].dropna().reset_index(drop=True)\n",
        "\n",
        "\n",
        "def read_dat_delimited_column(path: Path, column: str, sep: Optional[str] = None) -> pd.DataFrame:\n",
        "    \"\"\"Read a .dat file and return only the requested column.\n",
        "\n",
        "    Tries tab, comma, and pipe separators if none is provided.\n",
        "    \"\"\"\n",
        "    logger.info(\"Loading DAT: {} (column={})\", path, column)\n",
        "    seps = [sep] if sep else [\"\\t\", \",\", \"|\"]\n",
        "    last_err: Optional[Exception] = None\n",
        "    for s in seps:\n",
        "        try:\n",
        "            df = pd.read_csv(path, sep=s)\n",
        "            if column in df.columns:\n",
        "                return df[[column]].dropna().reset_index(drop=True)\n",
        "            last_err = KeyError(f\"Column '{column}' not in columns parsed with sep '{s}'\")\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "    raise RuntimeError(f\"Failed to parse {path} for column '{column}'. Last error: {last_err}\")\n",
        "\n",
        "\n",
        "def read_sdf_smiles(path: Path, smiles_field: str = \"canonical_smiles\") -> pd.DataFrame:\n",
        "    \"\"\"Read an SDF file and extract SMILES from a specified property or from the molecule.\"\"\"\n",
        "    if not _HAS_RDKIT:\n",
        "        raise ImportError(\"RDKit is required to parse SDF files. Please install rdkit-pypi.\")\n",
        "    logger.info(\"Loading SDF: {} (field={})\", path, smiles_field)\n",
        "    supplier = Chem.SDMolSupplier(str(path), removeHs=False)  # type: ignore[attr-defined]\n",
        "    smiles_list: List[str] = []\n",
        "    for mol in tqdm(supplier, desc=f\"Reading {path.name}\"):\n",
        "        if mol is None:\n",
        "            continue\n",
        "        value = mol.GetProp(smiles_field) if mol.HasProp(smiles_field) else None\n",
        "        if not value:\n",
        "            try:\n",
        "                value = Chem.MolToSmiles(mol)  # type: ignore[attr-defined]\n",
        "            except Exception:\n",
        "                value = None\n",
        "        if value:\n",
        "            smiles_list.append(value)\n",
        "    return pd.DataFrame({\"canonical_smiles\": smiles_list}).dropna().reset_index(drop=True)\n",
        "\n",
        "\n",
        "def read_glycan_csv_fourth_column(path: Path) -> pd.DataFrame:\n",
        "    \"\"\"Read a CSV and return the 4th column (index 3).\"\"\"\n",
        "    logger.info(\"Loading Glycan CSV: {} (4th column)\", path)\n",
        "    df = pd.read_csv(path, header=0)\n",
        "    if df.shape[1] < 4:\n",
        "        raise ValueError(f\"Expected at least 4 columns in {path}, got {df.shape[1]}\")\n",
        "    col_name = df.columns[3]\n",
        "    out = df[[col_name]].dropna().reset_index(drop=True)\n",
        "    out.columns = [\"glycan_field\"]\n",
        "    return out\n",
        "\n",
        "\n",
        "def read_rips_from_tar_json(path: Path, json_field: str = \"translation\") -> pd.DataFrame:\n",
        "    \"\"\"Extract a field from all JSON files inside a tar archive into a DataFrame.\"\"\"\n",
        "    logger.info(\"Loading RIPs from TAR JSON: {} (field={})\", path, json_field)\n",
        "    values: List[str] = []\n",
        "    with tarfile.open(path, \"r\") as tf:\n",
        "        members = [m for m in tf.getmembers() if m.isfile() and m.name.lower().endswith(\".json\")]\n",
        "        for m in tqdm(members, desc=f\"Reading {path.name}\"):\n",
        "            f = tf.extractfile(m)\n",
        "            if f is None:\n",
        "                continue\n",
        "            try:\n",
        "                data = json.load(f)\n",
        "                if isinstance(data, dict) and json_field in data and data[json_field]:\n",
        "                    values.append(str(data[json_field]))\n",
        "            except Exception as e:\n",
        "                logger.warning(\"Failed to parse {} inside {}: {}\", m.name, path.name, e)\n",
        "            finally:\n",
        "                f.close()\n",
        "    return pd.DataFrame({json_field: values}).dropna().reset_index(drop=True)\n",
        "\n",
        "logger.info(\"Data loader utilities ready.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define paths\n",
        "SM = Path(\"/fsx/data/raw/drugbank/DrugBank_SM_drugs.csv\")\n",
        "oligos = Path(\"/fsx/data/raw/DNA.RNA_seq/random_DNA_RNA_sequences_10000.csv\")\n",
        "can_peptides = Path(\"/fsx/data/raw/TPDB/main.xls\")\n",
        "noncan_peptides = Path(\"/fsx/data/raw/NCPbook_noncanonicals/NCP.book_Homo_sapiens.dat\")\n",
        "cyclic_pep_lariat = Path(\"/fsx/data/raw/CycPeptMPDB/CycPeptMPDB_Peptide_Shape_Lariat.csv\")\n",
        "cyclic_pep_circle = Path(\"/fsx/data/raw/CycPeptMPDB/CycPeptMPDB_Peptide_Shape_Circle.csv\")\n",
        "nat_prod = Path(\"/fsx/data/raw/supernatural/supernatural3-11-2025.sdf\")\n",
        "glycans = Path(\"/fsx/data/raw/Glytoucan/glycan.csv\")\n",
        "RIPs = Path(\"/fsx/data/raw/mibig/mibig_json_4.0.tar\")\n",
        "\n",
        "# Load datasets into DataFrames\n",
        "logger.info(\"Starting dataset imports...\")\n",
        "\n",
        "# Approved drugs (SMs): column 'moldb_smiles'\n",
        "df_sm = read_csv_column(SM, \"moldb_smiles\")\n",
        "\n",
        "# Oligos/Nucleotides: column 'Sequence'\n",
        "df_oligos = read_csv_column(oligos, \"Sequence\")\n",
        "\n",
        "# Peptides (canonical): column 'Sequence'\n",
        "df_can_peptides = read_xls_column(can_peptides, \"Sequence\")\n",
        "\n",
        "# Non-canonical peptides: column 'NCP_sequence'\n",
        "df_noncan_peptides = read_dat_delimited_column(noncan_peptides, \"NCP_sequence\")\n",
        "\n",
        "# CycPeptMPDB (Macrocycles-lariat): column 'SMILES'\n",
        "df_cyclic_pep_lariat = read_csv_column(cyclic_pep_lariat, \"SMILES\")\n",
        "\n",
        "# CycPeptMPDB (Macrocycles-circular): column 'SMILES'\n",
        "df_cyclic_pep_circle = read_csv_column(cyclic_pep_circle, \"SMILES\")\n",
        "\n",
        "# Natural products: SDF field 'canonical_smiles'\n",
        "df_nat_prod = read_sdf_smiles(nat_prod, smiles_field=\"canonical_smiles\")\n",
        "\n",
        "# Glycans: 4th column\n",
        "df_glycans = read_glycan_csv_fourth_column(glycans)\n",
        "\n",
        "# RIPs: 'translation' field in each JSON inside tar\n",
        "df_rips = read_rips_from_tar_json(RIPs, json_field=\"translation\")\n",
        "\n",
        "# Basic summary\n",
        "summary = {\n",
        "    \"df_sm\": len(df_sm),\n",
        "    \"df_oligos\": len(df_oligos),\n",
        "    \"df_can_peptides\": len(df_can_peptides),\n",
        "    \"df_noncan_peptides\": len(df_noncan_peptides),\n",
        "    \"df_cyclic_pep_lariat\": len(df_cyclic_pep_lariat),\n",
        "    \"df_cyclic_pep_circle\": len(df_cyclic_pep_circle),\n",
        "    \"df_nat_prod\": len(df_nat_prod),\n",
        "    \"df_glycans\": len(df_glycans),\n",
        "    \"df_rips\": len(df_rips),\n",
        "}\n",
        "logger.info(\"Import summary: {}\", summary)\n",
        "summary\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
